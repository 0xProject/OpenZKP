# Polynomials

## Fast extrapolation from ⟨ω⟩

Given $P ∈ \F[X]$ with $\deg P < n$ and $ω$ a $n$-primitive root of unity. Evaluate $P$ on the subgroup generated by $ω$:

$$
a_i = P(ω^i)
$$

**Goal.** *Given only $a_i$ and arbitrary $z ∈ \F$, efficiently evaluate $P(z)$.*

---

Observe the Lagrange basis for $⟨ω⟩$:

$$
L_i(X) = c_i \frac{X^n - 1}{X-ω^i}
$$

where $c_i$ is set such that $L_i(ω^j) = \delta_{ij}$ (i.e. one when $i=j$ and zero otherwise). Using L'Hospital rule we find:

$$
\left. \frac{X^n - 1}{X-ω^i} \right\vert_{X=ω^i} =
\left. \frac{n⋅X^{n-1}}{1} \right\vert_{X=ω^i} =
n ⋅ ω^{i(n-1)} =
\frac{n}{ω^i}
$$

and thus

$$
L_i(X) = \frac{ω^i}{n} \frac{X^n - 1}{X-ω^i}
$$

---

$$
\begin{aligned}
P(z) &= 
\sum_i a_i ⋅ L_i(z) \\\\&=
\sum_i a_i ⋅ \frac{ω^i}{n} \frac{z^n - 1}{z - ω^i} \\\\&=
\frac{z^n - 1}{n} \sum_i \frac{a_i ⋅ ω^i}{z - ω^i}
\end{aligned}
$$

This can be evaluated in $\mathcal{O}(1)$ space and $\mathcal{O}(m + \log n)$ time, where $m$ is the number of non-zero entries in $\vec a$.

## L'Hospital rules in finite fields

$$
\gdef\diff#1{\frac{\delta}{\delta #1}}
$$

**Warning.** "I have only proved it correct, not tried it."

Given $P,Q ∈ \F[X]$. Consider the fraction

$$
\frac{P(X)}{Q(X)}
$$

If this fraction is $\frac 00$ indeterminate in some value $α ∈ \F$, we can elliminate the zeros we found:

$$
\frac{P(X)/(X - α)}{Q(X)/(X - α)}
$$

If necessary we can repeat this untill the fraction is no longer indeterminate.

In real numbers, there is an alternative trick, where instead we differentiate the numbers:

$$
\frac{\diff X P(X)}{\diff X Q(X)}
$$

again, repeated as necessary. This trick can be applied in Finite Fields too, if we use formal derivatives.

Intuition suggest that L'Hospital's rule should give the same result at $X = α$. This essentially boils down to the following identity:

---

**Lemma.** *Given $P,Q ∈ \F[X]$ and $α ∈ \F$, then*

$$
\left. \frac{\diff X P(X)}{\diff X Q(X)} \right\vert_{X = α} =
\left. \frac{P(X)/(X - α)}{Q(X)/(X - α)} \right\vert_{X = α}
$$

*Proof.* Follows directly from below lemma.

---

**Lemma.** *Given $P ∈ \F[X]$ and $α ∈ \F$ with $P(α) = 0$ then*

$$
\left. \diff X P(X) \right\vert_{X = α} =
\left. \frac{P(X)}{X - α} \right\vert_{X = α}
$$

*Proof.* Substitute $X = U + α$ with $\diff X = \frac{\delta U}{\delta X}\diff U = \diff U$:

$$
\left. \diff U P(U + α) \right\vert_{U = 0} =
\left. \frac{P(U + α)}{U} \right\vert_{U = 0}
$$

Write $S(U) = P(U + α)$ such that $S(0) = 0$ and $S(U) = s_1 U + s_2 U^2 + ⋯$.

$$
\begin{aligned}
\left. \diff U S(U) \right\vert_{U = 0} &=
\left. \frac{S(U)}{U} \right\vert_{U = 0}
\\\\
\left. \diff U \left(s_1 U + s_2 U^2 + ⋯ \right) \right\vert_{U = 0} &=
\left. \frac{s_1 U + s_2 U^2 + ⋯}{U} \right\vert_{U = 0}
\\\\
\left. \left(s_1 + 2 s_2 U + 3 s_3 U^2 ⋯ \right) \right\vert_{U = 0} &=
\left. \left(s_1 + s_2 U + s_3 U^2 ⋯ \right) \right\vert_{U = 0}
\\\\
s_1  &= s_1
\end{aligned}
$$
□

**Note.** The left-hand side expression does not depend on $α$, so the differentiated polynomial will give the divided out values wherever $P(X) = 0$. $P$ of $\deg P = n$ has at most $n$ roots and interpolates $n + 1$ points. $P'(X)$ has $\deg P' = n - 1$ and interpolates the $n$ roots and their associated divided out values.

**Corrolary.** *The formal derivative of a polynomial is the polynomial that interpolates all the 'divided out zeros'.*

**To do.** What if the the original polynomial contains zeros of higher multiplicity?

**To do.** What if the original polynomial contains irreducible factors of higher degree?

---

The above suggest a more general theorem:

**Lemma.** *Given $P ∈ \F[X]$ and $z ∈ \F$, the following holds:*

$$
\left. \diff X P(X) \right\vert_{X = z} =
\left. \frac{P(X) - P(z)}{X - z} \right\vert_{X = z}
$$

---

$$
P(X) = \prod_i (X - a_i)
$$

Can this be evaluated using dual numbers? See https://en.wikipedia.org/wiki/Dual_number .

Evaluate over $\F[\bar X,Ε]/(Ε^2)$ with $X = \bar X + Ε$:

$$
P(X) = \prod_i (\bar X + Ε - a_i)
$$

The result will be $P(\bar X) + \left(\diff {\bar X} P(\bar X)\right) E$, i.e. the first derivative will be the coefficient of $E$.

Using a higher cut-off $\F[\bar X,Ε]/(Ε^n)$, higher order derivates can be obtained.

The division function can be modified to automatically apply L'Hospital's rule when it faces a $\frac 00$ indeterminate. This allows an evaluation strategy that would normally fail suceed anyway when executed over the dual numbers.

Take for example the $i$-th Lagrange interpolants over $⟨ω⟩$

$$
\frac{X^n - 1}{X - ω^i}
$$

## Efficient patterns of zeros of certain polynomials

### Context

In STARKs the computation is unrolled into a table:

| $x$ | $P_1(x)$ | $P_2(x)$ | $\dots$ | $P_n(x)$ |
|----|----|----|----|----|
| $\omega^0$ | a | b | $\cdots$ | c |
| $\omega^1$ | d | e | $\cdots$ | f |
| $\vdots$   | $\vdots$ | $\vdots$ | $\ddots$ | $\vdots$ |
| $\omega^{n-1}$ | g | h | $\cdots$ | i |

Think of the columns as registers and the rows as sequential states in the computation.

The columns in table are interpreted as polynomials evaluated on powers of a root of unity $\omega$.

Constraints are now expressed as rational functions. Consider the constraint that $P_2$ is the sum of previous $P_1$ and $P_2$ (Fibonacci constraint):

$$
\frac{
    P_2(\omega\cdot x) - P_1(x) - P_2(x)
}{
    (x^n - 1)/(x - \omega^{n-1})
}
$$

The numerator is an expression that is zero whenever the constraint holds between two rows. The denominator is zero whenever the constraint *should* hold. The division is exact only when the trace table is valid.

In STARKs, the verifier needs to evaluate the constraints once, given values of $P_i$. For an efficient proof system, this the expression should be evaluable in complexity at most logarithmic in $n$.

The above can be evaluated in $O(\log n)$ using binary exponentiation.

### Statement

Consider a large prime field $\mathbb{F}_p$ with a $n = 2^k$ order root of unity $\omega$. That is, $\omega^n = 1$.

We are interested in constructing efficient polynomials which have their zeros at certain powers of this root. Efficient here means it can be evaluated in $O(\log(n))$ given arbitrary preprocessing.

Which patterns of zeros can be efficiently computed?

### Positive examples

* $(x - ω^i)$  is zero at $ω^i$. It can be evaluated in $O(1)$.
* $(x^n - 1)$ is zeros at all powers of $ω$. It is efficient because with repeated squaring it can be evaluated in $O(log(n))$.
* $(x^{n/m} - 1)$ is zero at every $m$-th power of $ω$.

### Combinators

* $A(x) \cdot B(x)$ is zero whenever $A$ or $B$ is zero. This is efficient if $A$ and $B$ are. Be careful that overlap changes multiplicity.
* $A(x) / B(x)$ is zero whenever $A$ is zero, except when $B$ is zero assuming the multiplicity of the zeros is one.
* $A(ω^i x)$ has all zeros moved by $i$ places.

### Open questions

Is there an efficient evaluation of the polynomial that is zero at $\omega^0, \dots, \omega^{n/2}$, i.e. only the first half of the table.

What about arbitrary ranges?

### Prelimiary results

* Reed-Solomon theory will explain which patterns result in sparse polynomials. The example problem will result in a dense polynomial. Takeaway: the example problem will require a clever way to evaluate a dense polynomial. We know these exist for some polynomials. (from discussion with Dimitry)

* The general problem without precomputation is unsolvable. As we let $N$ grow to infinity, the number of patterns grows $2^n$, but the number of efficient evaluation circuits grows as $\log n$. This assumes we don't use constants besides $1$. Takeaway: Any generic solution will require more than $\log n$ precomputation for the constants. (from discussion with Dan)

### Appendix: why is this field interesting

Arithmetic circuits are a generalization of boolean circuits. Finite fields offer richter math than booleans and some researcher believe this has opertunities for solving hard complexity theory problems.

I also believe the results in the field are not as widely known as they should be, for example, many people believe Horner's evaluation is optimal. Given a polynomial

$$
P(x) = c_0 + c_1 x^1 + c_2 x^2 + \cdots  + c_n x^n
$$

Horner's schema allows this to be evaluated using $n$ multiplications and $n$ additions:

$$
\begin{aligned}
p_0 &= 1 \\\\
p_{i+1} &= p_{i} \cdot x + c_{n-i} \\\\
P(x) &= p_n
\end{aligned}
$$

Which takes $n$ multiplications. But [Rabin-Winograd (1972)][rw72] showed that any polynomial can be evaluated in $\frac{n}{2} + \log n$ multiplications. In the following, assume $n = 255$. The method generalizes for arbitrary degree in the paper.

[rw72]: https://doi.org/10.1002/cpa.3160250405

We first turn $P$ monic by dividing by the leading term, in the end we will multiply by it again:

$$
c'_i = \frac{c_i}{c_n}
$$

$$
P(x) = c_n (c'_0 + c'_1 \cdot x + c'_2 \cdot x^2 + \dots + x^n)
$$

Now we pick $i = \frac{n + 1}{2} = 128$ and split the polynomial in two:

$$
P(x) = Q(x) \cdot (x^{128} + a) + R(x)
$$

Where $Q$ and $R$ are monic with degree $127$. The coefficients of $Q$ are simply $c'_{129}, \dots, c'_{255}$. The value of $a$ is $c_{128} - 1$. From this we can compute the coefficients of $R$, they are $r_i = c'_i - c \cdot q_i$ and $r_{127} = 1$.

We apply this splitting recursively untill we are left with many monic polynomials of degree 3: $S(x) = s_0 + s_1 x + s_2 x^2 + x^3$. These are computed using:

$$
S(x) = (x^2 + s_1) \cdot (x + s_2) + b
$$

where $b$ is $s_0 - s_1 s_2$. All in all, the method requires about $n/2$ multiplications, about half of Horners method. To this we need to add the $\log n$ squarings required to get the required powers of $x$.


---

With $N$ a power of two:

$$
P(X) = (X - \omega_N^0) (X - \omega_N^1) \cdots (X - \omega_N^{N/2})
$$

$$
P(X) = (X - 1) (X - \omega_N)(X - \omega_N^2) \cdots (X + 1)
$$

**Conjecture.** *In $\mathbb C$ the coefficients of $P$ alternate between real multiples of $\omega_4^0, \omega_4^1, \omega_4^2, \omega_4^3$.*

**Conjecture.** *Furthermore, the scalar factors are symmetric in the sens that $c_i = c_{N/2 - i}$.*

**Conjecture.** *Furthermore, the scalar factors follow a bell curve.*

**To do.** Propose concrete formula for coefficients.

**To do.** This seems to generalize to composite $N$.


*Symmetries*: Take $P(X)$ to be the polynomial with zeros on $\omega_N^0, \dots \omega_N^{N/2}$. It should have the following symmetries based on observations of patterns of roots:

* Translation by one

    $$
    (X - \omega_N^0) P(\omega_N X) = (X - \omega_N^{N/2 + 1}) P(X)
    $$

    and observing that $\omega_N^{N/2 + 1} = \omega_N \cdot \omega_N^{N/2} = -\omega_N$:

    $$
    (X - 1) P(\omega_N X) = (X + \omega_N) P(X)
    $$


* Complement.

    $$
    P(X)P(-X) = (X+1)(X-1)(X^N -1)
    $$

    $$
    P(X)P(-X) = (X^2-1)(X^N -1)
    $$

* (Conjectured)

    $$
    P(X) = P\left(\frac{\omega_N}{X}\right)X^{N/2}
    $$



---

<https://helper.ipam.ucla.edu/publications/ccgtut/ccgtut_11787.pdf>

<https://en.wikipedia.org/wiki/Vieta%27s_formulas>

<https://en.wikipedia.org/wiki/Gauss%E2%80%93Lucas_theorem>

<https://en.wikipedia.org/wiki/Geometrical_properties_of_polynomial_roots>

<https://en.wikipedia.org/wiki/Polynomial_decomposition>

---

Decomposition:

**To do**. Try decomposition methods. Observe that $X^{2^k}-1 = (X^2 -1) \circ (X^2) \circ \cdots \circ (X^2)$. The rhs can be evaluated in $O(k)$ operations.

<https://web.archive.org/web/20150924101735/http://www.sigsam.org/bulletin/articles/187/Polynomial_time_decomposition_pp13-23.pdf>

<https://arxiv.org/pdf/1107.0687.pdf>

Does not seem to work for small examples: <https://www.wolframalpha.com/input/?i=Decompose%5B%28x-i%29%28x-1%29%28x%2B1%29%2C+x%5D even though $X^4-1$ works https://www.wolframalpha.com/input/?i=Decompose%5B%28x-i%29%28x-1%29%28x%2B1%29%28x%2Bi%29%2C+x%5D>

---

Interpolation

$$
\begin{aligned}
P(z) &= 
\sum_i a_i ⋅ L_i(z) \\\\&=
\sum_i a_i ⋅ \frac{ω^i}{n} \frac{z^n - 1}{z - ω^i} \\\\&=
\frac{z^n - 1}{n} \sum_i \frac{a_i ⋅ ω^i}{z - ω^i}
\end{aligned}
$$

where $a_i = \begin{cases} 0 & i \le N/2 \\ \ne 0 & otherwise \end{cases}$.

$$
\frac{z^n - 1}{n} \sum_{i \in (N/2, N)} \frac{a_i \cdot ω^i}{z - ω^i}
$$

This reduces the problem to computing the sum in $\log N$ time, plus we are allowed to multiply each term of the sum by a nonzero factor of our choosing. First of, let's absorb the other factors in $a_i$:

$$
(z^n - 1) \sum_{i \in (N/2, N)} \frac{a_i}{z - ω^i}
$$

The integral related to the sum seems solvable: <https://www.wolframalpha.com/input/?i=Integrate%5B1%2F%28a-b%5Ex%29%2C+x%5D>

The series seems related to the digamma function: <https://en.wikipedia.org/wiki/Digamma_function#Evaluation_of_sums_of_rational_functions http://mathworld.wolfram.com/q-PolygammaFunction.html>

There is potentially a closed form solution, at least for complex numbers: 

<https://www.wolframalpha.com/input/?i=Sum%5B1%2F%28z-b%5Ei%29%2C+%7Bi%2C+n+%2C++2*n%7D%5D>

<https://en.wikipedia.org/wiki/Lambert_series>

## Composition

**Context.** We are given a prime field $\F_p$, a size parameter $n$ and the following:

* $\log_2 p > 250$ (the prime is of *cryptographic size*),
* $2^n \,\vert\, p - 1$ (the field has roots of unity $\omega_n$).
* $n = 2^k < 2^{28}$ (we can realistically compute operations that ar $O(n \log n)$, but not $O(n ^2)$).

**Note.** If it helps, you may assume additional restricions on the prime $p$, for example the existance of other roots of unity. (Depending on how common these primes are, the result may no longer be relevant in pairing cryptography, but it will still be relevant for FRI and DARK based constructions). You may even outright pick a prime.

We denote the set of polynomials of degree less than $k$ as $\F_{< k}[X]$.

As is common in Cryptography, we accept probabilistic results as long as the probability is larger than $1 - 2^{\lambda}$, where $\lambda$ is the number of *bits of security*. We typically require at least $\lambda > 80$, but higher is better.

Finally, we are working in interactive proofs where there is dialogue between a prover and a verifier. Given a polynomial $P \in \F_{< n}$ we have existing protocols that allow us to send an *oracle* for this polynomial to the verfier, and the verifier can then ask for evaluations of these polynomials. Note that this existing protocol only works for polynomials of degree less than $n$.

---

**Lemma.** (Polynomial identity test). Given two polynomials $P, Q \in \F_{< n}[X]$ we want to convice the verifier that they are equal. The prover sends oracles for $P$ and $Q$ to the verifier, the verifier responds by asking for evaluations on a random point $\alpha$ and checks that they are equal.

**Proof.** By the Schwartz-Sippel lemma.

---

**Problem.** (Efficient zeros). An *efficiently evaluable polynomial* of degree $n$ has an $\O(\log n)$ sized circuit of $+$ or $\times$ operations that evaluates it. We are looking for efficiently evaluable polynomials that have as roots a subbset of the roots of unity $\omega_n^i$.

For example $X^n -1$ is efficiently evaluable and has all the roots of unity as roots. $X^{n/2} -1$ is efficient and has a roots all even powers of $\omega_n^i$.

---

**Problem.** (Composition) Given $F,G,H \in \F_{< n}[X]$, we want to interactively proof that

$$
F(G(X)) \\!\\!\\!\\!\mod H(X) = 0
$$

We can send *oracles* to the verifier, but only for polynomials in $\F_{\le n}$.

The current proof protocols use

$$
F(G(X)) = H(X) Z(X)
$$

and then writes

$$
Z(X) = Z_0(X^n) + X \cdot Z_1(X^n) + \cdots + X^n Z_n(X^n)
$$

And sends oracles for $Z_0 \dots Z_n$ and uses identity testing. The problem is this uses $n$ operations, and we'd like a protocol that uses at most $\O(\log n)$ exchanges.

**Restrictions.** Feel free to assume any of

* $H(X) = X^n -1$ or any other efficient polynomial.
* $F(X) = X^n -1$ or any polynomial.

**Generalizations.** Same problem, but now with $F$ a multivariate over multiple $G$

$$
F(G_0(X), G_1(X), G_2(X), \dots, G_) = H(X) Z(X)
$$

---

Solution by *Reid Barton* for $F$ an efficient polynomial.

$$
\begin{aligned}
G_1(X) - G(X)^2 &= H(X) Z_1(X) \\\\
G_2(X) - G(X)^2 &= H(X) Z_2(X) \\\\
& \,\,\,\vdots \\\\
G_k(X) - G(X)^2 &= H(X) Z_k(X) \\\\
\end{aligned}
$$

## Bivariate polynomials

$$
\def\F{\mathbb F}
$$

**Goal.** Simulate a form of (random access) memory using bivariate polynomials $P\in\F[X,Y]$.

Intuitively, we lay out the memory with time on the $X$ axis and space (addresses) on the $Y$ axis.

---

## Stack

Start with a simple stack where $P(ω_n^i, ω_m^j)$ is the value of stack location $j$ at time $i$.

The initial polynomial is the zero polynomial on our basis $ω^j$

$$
P(0, Y) = Y^m - 1
$$

**Note.** It looks like we can handle the memory sparsely, both in the polyonmial form and in the merkle trees.


$$
L_{m,j}(Y) = \frac{ω_m^j}{m} \frac{Y^m - 1}{Y-ω_m^j}
$$

### Push & Pop

**Constraint.** *(Push). Rotate all elements to the right, add an element $x$. Has the following constraints:*

$$
\begin{aligned}
P(X, ω_m^{m-1}) &= 0 \\\\
P(ω_n ⋅ X, Y) &= P(X, ω_m ⋅ Y) + x ⋅ L_0(Y)
\end{aligned}
$$

**Constraint.** *(Pop). Operate the push constraint in reverse.*

### Random access

**Constraint.** *(Compare and swap). Replace the value at $j$ from $a$ to $b$.*

$$
\begin{aligned}
P(X, ω_m^j) &= a \\\\
P(ω_n ⋅ X, Y) &= P(X, Y) + (b - a) ⋅ L_j(Y)
\end{aligned}
$$

The problem here is that we need to encode the $j$ somehow, either in a precomputed polynomial or as a column. Since all usage of $j$ will be in the form $ω_m^j$ we will use that instead. the final constraint will look like:

$$
\begin{aligned}
P(X, Q(X)) &= a \\\\
P(ω_n ⋅ X, Y) &= P(X, Y) + (b - a) ⋅ \frac{Q(X)}{m} \frac{Y^m - 1}{Y-Q(X)}
\end{aligned}
$$

**To do.** What if $Q(X)$ is not of the form $ω_m^j$?

The first constraint has a degree bound of $(\deg P)⋅(\deg Q) = n^2 m$. So far we have only handle constant constraint bounds. How do we LDE test this efficiently?

> We could add a temporary value? $t = Q(X), P(X, t) = a$?

In addition, to force $Q(X)$ to be a root of unity, we can add a constraint

$$
Q(X)^m = 1
$$

which has degree bound $m\deg Q = nm$. Again, how do we LDE-test this efficiently?

---

## Polynomial composition

Both approaches above hit the problem that we need to low-degree-test an expression of the form $P(Q(X))$. The straightforward approach has degree bound $(\deg P)(\deg Q)$ which is quadratic. This will kill prover performance and more than double proof size.

**Goal.** *Find a way to LDE test polynomial compositions efficiently.*

**To do.** Specify goal more concretely.

https://en.wikipedia.org/wiki/Polynomial_decomposition

https://www.math.mcgill.ca/rickards/PDFs/amer.math.monthly.118.04.358-rickards.pdf

https://www.cs.cornell.edu/~kozen/Papers/poly.pdf

Chebyshev polynomials have the nesting property $T_n(T_m(X)) = T_{mn}(X)$.

https://dspace.mit.edu/bitstream/handle/1721.1/71792/Kedlaya-2011-FAST%20POLYNOMIAL%20FACT.pdf?sequence=1&isAllowed=y
https://www.cse.iitk.ac.in/users/nitin/courses/CS681-2016-17-II/pdfs/slides-dwivedi.pdf
Evaluates $f(g(x)) \mod h(x)$ in less then $\mathcal O(n^2)$ time using FFTs. => Read.
http://users.cms.caltech.edu/~umans/papers/U07.pdf

https://www.cse.iitk.ac.in/users/nitin/courses/CS681-2016-17-II/pdfs/slides-dwivedi.pdf

https://citeseer.ist.psu.edu/viewdoc/download;jsessionid=611B98690C1028968AED2736F9E1E77C?doi=10.1.1.51.3154&rep=rep1&type=pdf

https://arxiv.org/pdf/0807.3578.pdf


---


Aside: https://en.wikipedia.org/wiki/Bruun's_FFT_algorithm

## Composition proof

$$
\def\F{\mathbb F}
$$

**Context.** Are values are from a 

**Goal.** *Given $P\in\F_{< N}[X]$ Proof the following constraint using Schwartz-Sippel and a $\le N$ low degree test:*

$$
F(G(X)) \mod H(X) = 0
$$



$$
\frac{P(X)^{2^{64}} - 1}{X^N - 1}
$$

**Lemma.** *Polynomials satisfying the above constraint satisfy $P(\omega_N^i) = \omega_{2^{64}}^{k_i}$ for $i \in [0,N)$.*

**Definition.** *$\F_{< N}[X] \sim \F[X]/ X^N$*

---

Assume for the moment $N = 2^{64}$, we can generalize later

$$
\frac{P(X)^N - 1}{X^N - 1}
$$

This puts some constraints on the coefficients of $P$.

$$
P(X) = \sum_{i\in[0,N)} p_i X^i
$$

$$
\frac{P(X)^N - 1}{X^N - 1} =
\frac{\left(\sum_{i\in[0,N)} p_i X^i\right)^N - 1}{X^N - 1}
$$

We can now expand using something like the https://en.wikipedia.org/wiki/Binomial_theorem, i.e. https://en.wikipedia.org/wiki/Multinomial_theorem.

**To do.** Look into differentials.

### Simplifications

We can further restrict the problem to the case $h(x) = x^n - 1$. This can help because we can now factor $h$ as

$$
h(x) = (x - ω_n^0) (x - ω_n^1) (x - ω_n^2) \cdots (x - ω_n^{n-1})
$$

or (thanks Yan Zhang)

$$
h(x) = (x - 1) (x + 1) (x^2 + 1) (x^4 + 1) \cdots (x^n + 1)
$$

The problem could be broken solved modulo these factors and then invoke CRT.

---

### References

* J. F. Ritt (1921). "Prime and Composite Polynomials".
  [link](https://www.ams.org/journals/tran/1922-023-01/S0002-9947-1922-1501189-9/S0002-9947-1922-1501189-9.pdf).
* Raoul Blankertz (2014). "A polynomial time algorithm for computing all minimal decompositions of a polynomial"
  [link](https://web.archive.org/web/20150924101735/http://www.sigsam.org/bulletin/articles/187/Polynomial_time_decomposition_pp13-23.pdf)
* K. S. Kedlaya, C. Umans (2011). "Fast polynomial factorization and modular composition"
 [link](http://users.cms.caltech.edu/~umans/papers/KU08-final.pdf)
